# 5월 9일

IN 9-32

북반구에서는 summer_solstice와 june_solstice가 같은 말

15(summer_solstice)와 3(winter_solstice)는 반의어임

반의어를 나타내는 에지의 attention weight가 cycle encoder가 더 컸음 → 의미 있는 정보임

확실한 것은 cycle encoder의 attention weight들이 qagnn에 비해 큼

9-12 가 cycle encoder가 큼

22-1이 cycle encoder가 큼

일단 저 단어들의 뜻을 기록해놓자 

이 그래프에서 내가 생각하는(문제와 관련없는) negative cycle은 없다.

그렇지만  Cycle encoder의 attention weight가 QA-GNN의 attention weight보다 전체적으로 크다.

그리고 QA-GNN은 Cycle encoder에 비해 낮은 값들의 값이 비슷하다.

### 31 : 9-776-A 18개

- 0 : 3
- 1~ 4: 0 → last, saws, longer,lasts_longer
- 5,6,7 : 1 → see, iron, saw
- 8~ : 2
- text : Which of these saws will last longer?
- question node : "qc": ["last", "lasts_longer", "longer", "saws"],
- answer node : "ac": ["iron", "saw", "see"]

```
concept_ids tensor([     0,   7074, 239708, 317570, 625210,   2353,   9616,  27768, 610565,
           512,   4256,   5927,   2091,   3558,  10999,  13845,   5799,   3462,
          5355,  29657,   6696,   4352,   3016,   2064,  14663,  53934,   9275,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1,      1,      1,      1,      1,      1,      1,      1,
             1,      1])
edge_index파악하기 tensor([[ 1, 26,  6,  6,  7, 15, 18,  6,  2,  1,  2,  5,  5,  5,  5,  5,  6,  6,
          6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8, 10, 10, 10, 11, 12,
         12, 13, 13, 13, 13, 15, 15, 17, 19, 19, 20, 20, 20, 20, 20, 20, 20, 21,
         21, 21, 21, 22, 23, 23, 23, 24, 24, 24, 24, 24, 24, 25, 25, 26, 26, 26,
         17,  0,  0,  0,  0,  0,  0,  0,  9, 14, 10, 19, 19,  1,  5, 17, 16, 22,
          7, 14, 20, 21, 23, 25,  9, 10, 11, 13, 14, 15, 18, 19, 20, 10, 11, 16,
         19,  1,  3,  6, 11, 18, 18,  1,  3,  5, 10, 14, 18,  1, 18, 20,  7, 14,
          5, 10, 13, 14, 18, 19, 22,  3,  4, 13, 18,  3,  6, 18, 19,  3,  7, 14,
         18, 19, 22,  5,  7,  3,  4, 18,  5,  1,  2,  3,  4,  5,  6,  7],
        [ 9, 14, 10, 19, 19,  1,  5, 17, 16, 22,  7, 14, 20, 21, 23, 25,  9, 10,
         11, 13, 14, 15, 18, 19, 20, 10, 11, 16, 19,  1,  3,  6, 11, 18, 18,  1,
          3,  5, 10, 14, 18,  1, 18, 20,  7, 14,  5, 10, 13, 14, 18, 19, 22,  3,
          4, 13, 18,  3,  6, 18, 19,  3,  7, 14, 18, 19, 22,  5,  7,  3,  4, 18,
          5,  1,  2,  3,  4,  5,  6,  7,  1, 26,  6,  6,  7, 15, 18,  6,  2,  1,
          2,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,
          7,  8,  8, 10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 15, 15, 17, 19, 19,
         20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 22, 23, 23, 23, 24, 24, 24,
         24, 24, 24, 25, 25, 26, 26, 26, 17,  0,  0,  0,  0,  0,  0,  0]])
edge_index파악하기 torch.Size([2, 160])

edge_type파악하기 tensor([ 2,  2,  7,  7,  7,  9,  9, 10, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17,
        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
        18,  0,  0,  0,  0,  1,  1,  1, 21, 21, 26, 26, 26, 28, 28, 29, 31, 36,
        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,
        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,
        36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,
        36, 36, 36, 36, 36, 36, 36, 36, 37, 19, 19, 19, 19, 20, 20, 20])
edge_type파악하기 torch.Size([160])
```

![Untitled](5%E1%84%8B%E1%85%AF%E1%86%AF%209%E1%84%8B%E1%85%B5%E1%86%AF%20eb09e5b52e4d4600b7d534c0bc9c1b0f/Untitled.png)

- other node index
    - 10 : metal
    - 11 : hard
    - 13 : color
    - 15 : run
    - 16 : sharp
    - 17 : sun
    - 18 : like
    - 19 : tool
    - 20 : eye
    - 21 : picture
    - 22 : line
    - 23 : chair
    - 24 : stitch
    

![Untitled](5%E1%84%8B%E1%85%AF%E1%86%AF%209%E1%84%8B%E1%85%B5%E1%86%AF%20eb09e5b52e4d4600b7d534c0bc9c1b0f/Untitled%201.png)

edge_index파악하기 torch.Size([2, 160]) 0:160
edge_index파악하기 torch.Size([2, 60])
edge_index파악하기 torch.Size([2, 132])
edge_index파악하기 torch.Size([2, 86])

7(saw)→19(tool) : isa

cycle encoder : 0.24

qagnn : 0.21

7(saw)→19(tool) : relatedto

cycle encoder : 0.09

qagnn : 0.10

전체적으로 cycle 이 많은 노드로 향하는 attention 이 커짐 → 이거 상관관계 출력해보자