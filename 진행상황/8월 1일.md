# 8월 1일

./data/csqa/graph/train.graph.adj.pk 파일은 conceptnet으로 부터 추출되는 subgraph

하지만 이것은 context 노드를 추가하지 않음

context node를 추가하는 것은

./utils/data_utils.py의 load_sparse_adj_data_with_context_node로부터 생성됨

![Untitled](8%E1%84%8B%E1%85%AF%E1%86%AF%201%E1%84%8B%E1%85%B5%E1%86%AF%20c772238929b74871abc2ad8069abd60e/Untitled.png)

위 코드는 qagnn.py의 일부분이다.

여기서 반복문 부분에 dataset.train() 부분을 통해 input_data가 나오며 그 input_dat가 model의 입력으로 들어간다.

dataset.train()함수를 보면 다음과 같다.

![Untitled](8%E1%84%8B%E1%85%AF%E1%86%AF%201%E1%84%8B%E1%85%B5%E1%86%AF%20c772238929b74871abc2ad8069abd60e/Untitled%201.png)

return으로 MultiGPUSparseAdjDataBatchGenerator함수를 부른다.

이 함수는 utils/data_utils.py에 있으며 다음과 같다.

```python
class MultiGPUSparseAdjDataBatchGenerator(object):
    def __init__(self, args, mode, device0, device1, batch_size, indexes, qids, labels,
                 tensors0=[], lists0=[], tensors1=[], lists1=[], adj_data=None):
        self.args = args
        self.mode = mode
        self.device0 = device0
        self.device1 = device1
        self.batch_size = batch_size
        self.indexes = indexes
        self.qids = qids
        self.labels = labels
        self.tensors0 = tensors0
        self.lists0 = lists0
        self.tensors1 = tensors1
        self.lists1 = lists1
        # self.adj_empty = adj_empty.to(self.device1)
        self.adj_data = adj_data

    def __len__(self):
        return (self.indexes.size(0) - 1) // self.batch_size + 1

    def __iter__(self):
        bs = self.batch_size
        n = self.indexes.size(0)
        if self.mode=='train' and self.args.drop_partial_batch:
            print ('dropping partial batch')
            n = (n//bs) *bs
        elif self.mode=='train' and self.args.fill_partial_batch:
            print ('filling partial batch')
            remain = n % bs
            if remain > 0:
                extra = np.random.choice(self.indexes[:-remain], size=(bs-remain), replace=False)
                self.indexes = torch.cat([self.indexes, torch.tensor(extra)])
                n = self.indexes.size(0)
                assert n % bs == 0

        for a in range(0, n, bs):
            b = min(n, a + bs)
            batch_indexes = self.indexes[a:b]
            batch_qids = [self.qids[idx] for idx in batch_indexes]
            batch_labels = self._to_device(self.labels[batch_indexes], self.device1)
            batch_tensors0 = [self._to_device(x[batch_indexes], self.device0) for x in self.tensors0]
            batch_tensors1 = [self._to_device(x[batch_indexes], self.device1) for x in self.tensors1]
            batch_lists0 = [self._to_device([x[i] for i in batch_indexes], self.device0) for x in self.lists0]
            batch_lists1 = [self._to_device([x[i] for i in batch_indexes], self.device1) for x in self.lists1]

            edge_index_all, edge_type_all = self.adj_data
            #edge_index_all: nested list of shape (n_samples, num_choice), where each entry is tensor[2, E]
            #edge_type_all:  nested list of shape (n_samples, num_choice), where each entry is tensor[E, ]
            edge_index = self._to_device([edge_index_all[i] for i in batch_indexes], self.device1)
            edge_type  = self._to_device([edge_type_all[i] for i in batch_indexes], self.device1)

            yield tuple([batch_qids, batch_labels, *batch_tensors0, *batch_lists0, *batch_tensors1, *batch_lists1, edge_index, edge_type])
```

여기서 for a in range(0,n,bs): 부분에 cycpe triple을 출력하면 되지 않을까?

하지만 이 코드는 학습마다 불러오므로 사전 전처리코드는 아니다. 

모델 학습 실행 과정

[qagnn.py](http://qagnn.py) 실행 → qagnn.py에 있는 dataset= LM_QAGNN_DataLoader실행 → dataset.train() 반복문 실행 → dataset.train() 리턴값 MultiGPUSparseAdjDataBatchGenerator 함수 실행 → MultiGPUSparseAdjDataBatchGenerator함수의 반복문 실행 → qagnn.py에서 dataset.train()반복문 내부 실행

내일 토이 코드 작성하기